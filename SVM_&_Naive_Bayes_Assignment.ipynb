{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 :  What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "0sYDZt8zujEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Information Gain (IG) measures how much uncertainty (entropy) is reduced after splitting a dataset based on a feature.\n",
        "\n",
        " - It tells us which feature best separates the data\n",
        "\n",
        " - The feature with highest Information Gain is chosen for splitting\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùêºùê∫(ùëÜ,ùê¥)=ùê∏ùëõùë°ùëüùëúùëùùë¶(ùëÜ)‚àí‚àë‚à£ùëÜùë£‚à£‚à£ùëÜ‚à£ùê∏ùëõùë°ùëüùëúùëùùë¶(ùëÜùë£)IG(S,A=Entropy‚à£S‚à£Sv‚à£Entropy(Sv)\n",
        "\n",
        "Use in Decision Trees:\n",
        "\n",
        " - Used mainly in ID3 and C4.5\n",
        " - Helps build smaller, more accurate trees"
      ],
      "metadata": {
        "id": "CFo9RNHgvsYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths, weaknesses, and appropriate use cases.\n"
      ],
      "metadata": {
        "id": "jzBixvWUujs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Aspect      | Gini Impurity             | Entropy             |\n",
        "| ----------- | ------------------------- | ------------------- |\n",
        "| Formula     | (1 - \\sum p^2)            | (-\\sum p \\log_2 p)  |\n",
        "| Used in     | CART                      | ID3, C4.5           |\n",
        "| Speed       | Faster                    | Slower              |\n",
        "| Sensitivity | Less sensitive to changes | More sensitive      |\n",
        "| Output      | Lower computation         | More precise splits |\n",
        "\n",
        "**Use Case:**\n",
        "\n",
        " - Gini ‚Üí Faster training, large datasets\n",
        "\n",
        " - Entropy ‚Üí Better theoretical purity"
      ],
      "metadata": {
        "id": "S7R5YEwMwdos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "2WpTofZGun4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Pre-Pruning stops tree growth early to prevent overfitting.**\n",
        "\n",
        "Common techniques:\n",
        "\n",
        " - Max depth\n",
        "\n",
        " - Minimum samples split\n",
        "\n",
        " - Minimum samples per leaf\n",
        "\n",
        " - Minimum information gain\n",
        "\n",
        "Advantages:\n",
        "\n",
        " - Faster training\n",
        "\n",
        " - Prevents overfitting\n",
        "\n",
        " - Smaller trees"
      ],
      "metadata": {
        "id": "-BW-wCzHwmeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
      ],
      "metadata": {
        "id": "Cv3-oZZ1uplz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Decision Tree\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqbdIAXEw0Ab",
        "outputId": "aa9adb66-6b2f-4885-fda5-b8e85c6b08ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.02666667 0.         0.55072262 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "ithyr9NUusz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - SVM is a supervised learning algorithm used for classification and regression.\n",
        "\n",
        "Finds the optimal hyperplane\n",
        "\n",
        "Maximizes the margin between classes\n",
        "\n",
        "Works well with high-dimensional data"
      ],
      "metadata": {
        "id": "VC3F87Ysw4L8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "-xxC7eo6uuhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Kernel Trick allows SVMs to:**\n",
        "\n",
        " - Transform data into higher dimensions\n",
        "\n",
        " - Solve non-linear problems efficiently\n",
        "\n",
        "**Common kernels:**\n",
        "\n",
        " - Linear\n",
        "\n",
        " - Polynomial\n",
        "\n",
        " - RBF (Gaussian)\n",
        "\n",
        " - Sigmoid"
      ],
      "metadata": {
        "id": "5KVWIHyKw9LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "pTw0z0A6uwCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Linear SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, pred_linear))\n",
        "print(\"RBF Kernel Accuracy:\", accuracy_score(y_test, pred_rbf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCKjWqIaxHrb",
        "outputId": "04a9cee5-5f35-4401-ead3-eaf6dc21b2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9259259259259259\n",
            "RBF Kernel Accuracy: 0.6296296296296297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?"
      ],
      "metadata": {
        "id": "3kDzNl9YuxxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na√Øve Bayes is a probabilistic classifier based on Bayes‚Äô Theorem.\n",
        "\n",
        "It is called ‚ÄúNa√Øve‚Äù because:\n",
        "\n",
        " - It assumes all features are independent\n",
        "\n",
        " - This assumption is rarely true but works well in practice"
      ],
      "metadata": {
        "id": "N5lvydxUxOKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes"
      ],
      "metadata": {
        "id": "9o8IYXTruzg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Type           | Used For        | Example             |\n",
        "| -------------- | --------------- | ------------------- |\n",
        "| Gaussian NB    | Continuous data | Height, weight      |\n",
        "| Multinomial NB | Count data      | Text classification |\n",
        "| Bernoulli NB   | Binary data     | Spam detection      |\n"
      ],
      "metadata": {
        "id": "n7Ooryamxk8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy. Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "mB6XyyUIu1Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "BmJVL6sTxoVz",
        "outputId": "855bdc8e-6f1e-4115-c6b8-d267826cf97d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9532163742690059\n"
          ]
        }
      ]
    }
  ]
}